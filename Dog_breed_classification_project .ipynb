{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj4ZocvLFzjF"
      },
      "source": [
        "# **Multiclass dog breed classification**\n",
        "\n",
        "This notebook builds an end-to-end image classifier using TensorFlow and TensorFlow Hub.\n",
        "\n",
        "## 1. Problem\n",
        "\n",
        "Creating a machine learning model that recognizes and classifies dog breeds given an image of a dog.\n",
        "\n",
        "## 2. Data\n",
        "\n",
        "The data comes frome Kaggle's 'Dog Breed Identification' competition: https://www.kaggle.com/competitions/dog-breed-identification/data.\n",
        "\n",
        "It consists of a training set (10222 samples with labels) and a test set (10.4k samples) of images of dogs. Each image has a filename that is its unique id. The dataset comprises 120 breeds of dogs.\n",
        "\n",
        "## 3. Evaluation\n",
        "\n",
        "Model performance is evaluated through a file that contains the predicted classification probability of each sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEgTDFqxMfLr"
      },
      "source": [
        "## Workspace preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z--qi319EhsO"
      },
      "outputs": [],
      "source": [
        "#importing the necessary libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlUcI05KbwYC"
      },
      "source": [
        "## Getting the data ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkWBGrrnPBTJ"
      },
      "outputs": [],
      "source": [
        "#Importing the csv file that contains the labels of the samples in the training set\n",
        "labels_csv = pd.read_csv('/content/drive/MyDrive/Dog breed classification project/labels.csv')\n",
        "print(labels_csv.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toe5rbmHdfET"
      },
      "outputs": [],
      "source": [
        "#we have 120 breeds and 10222 unique samples\n",
        "print(labels_csv.describe())\n",
        "#We have no missing data\n",
        "print(labels_csv.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIJWPUQ2PD78"
      },
      "outputs": [],
      "source": [
        "#Number of samples of each breed\n",
        "labels_csv['breed'].value_counts().plot(kind='bar', figsize=(18,8))\n",
        "plt.title(\"Number of samples for each breed\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55mjWDWkcybS"
      },
      "outputs": [],
      "source": [
        "#Median number of samples per breed\n",
        "labels_csv['breed'].value_counts().median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHxn0V8Fgq1t"
      },
      "outputs": [],
      "source": [
        "#Visualizing one of the sample images\n",
        "from IPython.display import Image\n",
        "Image('/content/drive/MyDrive/Dog breed classification project/train/0021f9ceb3235effd7fcde7f7538ed62.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nvhzUdXhzw9"
      },
      "outputs": [],
      "source": [
        "#Creating a list of the paths to each of the images\n",
        "filenames = ['/content/drive/MyDrive/Dog breed classification project/train/' + names for names in labels_csv['id'] +'.jpg']\n",
        "#Fisrt 5 elements of the list\n",
        "filenames[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjVKFo9HiwLe"
      },
      "outputs": [],
      "source": [
        "#Verifying that the number of samples in the train folder are the same as in labels_csv\n",
        "import os\n",
        "len(os.listdir('/content/drive/MyDrive/Dog breed classification project/train')) == len(labels_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFXLHnllCHsK"
      },
      "source": [
        "## Preparing the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syM0UwwzreLW"
      },
      "outputs": [],
      "source": [
        "#turning the labels into an array\n",
        "labels = labels_csv['breed'].to_numpy()\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIpcUnnbCl7_"
      },
      "outputs": [],
      "source": [
        "#Find the unique label values\n",
        "unique_breeds = np.unique(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPfVB3hADLwS"
      },
      "outputs": [],
      "source": [
        "#Turn every label into a boolean array\n",
        "boolean_array = np.array([label == unique_breeds for label in labels])\n",
        "boolean_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvRKT96rGBxu"
      },
      "outputs": [],
      "source": [
        "boolean_array.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4vQKVonETIu"
      },
      "outputs": [],
      "source": [
        "print(labels[0])\n",
        "#code to find the index where a label is found in unique_labels\n",
        "print(np.where(labels[0]==unique_breeds))\n",
        "#Index where label occurs in boolean_array\n",
        "print(boolean_array[0].argmax())\n",
        "#How to one hot encode a boolean_array\n",
        "boolean_array[0].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X36rs6KRIpmf"
      },
      "outputs": [],
      "source": [
        "encoded_labels = boolean_array.astype(int)\n",
        "encoded_labels[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLtcF6nEMddd"
      },
      "source": [
        "## Setting up the training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQyHghV2Kofq"
      },
      "outputs": [],
      "source": [
        "X = filenames\n",
        "y = encoded_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAvkEpPTSn-F"
      },
      "source": [
        "We'll start with 1000 images and then take them all once we have evaluated model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XyjdJHZNAuf"
      },
      "outputs": [],
      "source": [
        "#Set the number of images for experimenting\n",
        "NUM_IMAGES = 1000 # @param {type:\"slider\", min:1000, max:10000, step:1000}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzLYZ0FwTeWO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES], y[:NUM_IMAGES], test_size=0.2, random_state=1)\n",
        "\n",
        "len(X_train), len(X_val), len(y_train), len(y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RAyVxpKVoXX"
      },
      "source": [
        "## Preprocessing images: turning images into tensors\n",
        "\n",
        "We will create a function that:\n",
        "1. Takes an image filepath as input.\n",
        "2. Uses tensorflow to read the file and save it as a variable.\n",
        "3. Turns the image (jpeg) into tensors.\n",
        "4. Normalizes the image.\n",
        "5. Resizes the image to a shape of (224,224). This is because the transfer learning model was trained with that shape.\n",
        "6. Return the modified image.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxFst6bBVv9n"
      },
      "outputs": [],
      "source": [
        "#Converting an image to a numpy array\n",
        "image = plt.imread(filenames[4])\n",
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l0dLtpdnohp"
      },
      "outputs": [],
      "source": [
        "#an image consists of red green and blue values between 0 and 225 for pixels\n",
        "image.max(), image.min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9f30U5qoUaM"
      },
      "outputs": [],
      "source": [
        "#Transforming the image into a tensor\n",
        "t_image = tf.constant(image)\n",
        "t_image[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgwgRC6ppWT8"
      },
      "source": [
        "### Preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5GcTfO2pbu5"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "def preprocess_image(filepath, size=IMG_SIZE):\n",
        "\n",
        "  #reading the image as a tensor of type \"string\"\n",
        "  image = tf.io.read_file(filepath)\n",
        "  #turn the jpeg image into tensors with three color channels (red, green, blue)\n",
        "  image = tf.image.decode_jpeg(image,channels=3)\n",
        "  #Normalizing the values\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  #Resizing the image\n",
        "  image = tf.image.resize(image,size=[IMG_SIZE,IMG_SIZE])\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyhHSbA9sDkB"
      },
      "outputs": [],
      "source": [
        "#preprocess example\n",
        "preprocess_image(filenames[1]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvFS63GrvRcj"
      },
      "source": [
        "## Turning the data into batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AiPTrYmtnLm"
      },
      "outputs": [],
      "source": [
        "#Creating a function that returns a tuple of (preprocessed_image, label)\n",
        "def get_image_label(filepath,label):\n",
        "  image = preprocess_image(filepath)\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSuURz22wpfm"
      },
      "outputs": [],
      "source": [
        "#Defining the batch size\n",
        "BATCH_SIZE = 32\n",
        "#Create a function to create data into batches\n",
        "def create_data_batches(X,y=None, batch_size= BATCH_SIZE, valid_data=False,test_data=False):\n",
        "  '''\n",
        "  Create batches of data out of image (X) and label (y) pairs.\n",
        "  It shuffles the data if it is the training data, and doesn't shuffle if\n",
        "  it's validation data. It also accepts test data as input (no labels).\n",
        "  '''\n",
        "  #If the data is the test dataset, we likely won't have labels:\n",
        "  if test_data:\n",
        "    print('Creating test data batches...')\n",
        "    data = tf.data.Dataset.from_tensor_slices(tf.constant(X)) #only filepaths, no labels\n",
        "    data_batch = data.map(preprocess_image).batch(batch_size)\n",
        "    print('batch created')\n",
        "    return data_batch\n",
        "\n",
        "  #If the data is a validation dataset, we don't need to shuffle it\n",
        "  if valid_data:\n",
        "    print('Creating validation data batches...')\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), #filepath\n",
        "                                               tf.constant(y))) #labels\n",
        "    #This time we use the get_image_label function because we are also working with labels\n",
        "    data_batch = data.map(get_image_label).batch(batch_size)\n",
        "    print('batch created')\n",
        "    return data_batch\n",
        "\n",
        "  else:\n",
        "    print('Creating training data batches...')\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y)))\n",
        "    #shuffling the data before mapping\n",
        "    #The shuffle is done before preprocessing because it is more computationally efficient this way\n",
        "    data = data.shuffle(buffer_size=len(X))\n",
        "    data_batch = data.map(get_image_label).batch(batch_size)\n",
        "    print('batch created')\n",
        "    return data_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hasN-YdGlvBy"
      },
      "outputs": [],
      "source": [
        "train_data = create_data_batches(X_train,y_train)\n",
        "val_data = create_data_batches(X_val,y_val, valid_data=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDBN1Qaelvht"
      },
      "outputs": [],
      "source": [
        "#Now our data is in a dataset\n",
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0s9GW8PvQOI"
      },
      "source": [
        "## Visualizing data batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f_cnN3lvSG5"
      },
      "outputs": [],
      "source": [
        "#Creating a function for viewing images in a data batch\n",
        "def show_images(images, labels):\n",
        "  '''\n",
        "  Displays a plot of 25 images and their labels from a data batch\n",
        "  '''\n",
        "  #Setup the figure\n",
        "  plt.figure(figsize=(12,12))\n",
        "  #Loop through 25\n",
        "  for i in range(25):\n",
        "    #Create subplots\n",
        "    ax = plt.subplot(5,5,i+1)\n",
        "    #Display image\n",
        "    plt.imshow(images[i])\n",
        "    #Add the label as the image title\n",
        "    plt.title(unique_breeds[labels[i].argmax()])\n",
        "    #Turning grid lines off\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SVblhLRvY-u"
      },
      "outputs": [],
      "source": [
        "#Unbatching the images\n",
        "train_images, train_labels = next(train_data.as_numpy_iterator()) #next takes the top batch of the iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_z01lsZRW0v"
      },
      "outputs": [],
      "source": [
        "len(train_images), len(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_IQlWUr66Sa"
      },
      "outputs": [],
      "source": [
        "#Using the visualization function on the train dataset\n",
        "show_images(train_images, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwwp0LG2flka"
      },
      "source": [
        "## Bulding a model\n",
        "\n",
        "Previous steps:\n",
        "* The input shape (our images shape, in the form of Tensors) of our model.\n",
        "* The output shape (image labels, in the form of Tensors) of our model.\n",
        "* The URL of the model we want to use from TensorFlow Hub - https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIJrdkQNf_iT"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti7J9PQjiDLY"
      },
      "outputs": [],
      "source": [
        "#Setting the input shape\n",
        "INPUT_SHAPE = [None, IMG_SIZE,IMG_SIZE,3]\n",
        "#Setting the output shape\n",
        "OUTPUT_SHAPE = len(unique_breeds)\n",
        "#Model URL\n",
        "MODEL_URL = \"https://www.kaggle.com/models/google/mobilenet-v2/frameworks/TensorFlow2/variations/140-224-classification/versions/2\"\n",
        "#first one: \"https://kaggle.com/models/google/mobilenet-v2/frameworks/TensorFlow2/variations/130-224-classification/versions/1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X2dbCtwiolj"
      },
      "source": [
        "Creating a function that:\n",
        "* Takes the input and output shapes and the model as parameters.\n",
        "* Defines the layers in a Keras model sequentially\n",
        "* Compiles the model\n",
        "* Builds the model\n",
        "* Returns the model\n",
        "\n",
        "Documentation: https://www.tensorflow.org/guide/keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SKGWmVGisT_"
      },
      "outputs": [],
      "source": [
        "#Creating a function that creates a Keras model\n",
        "\n",
        "def create_model(model_url=MODEL_URL, input_shape=INPUT_SHAPE,output_shape=OUTPUT_SHAPE):\n",
        "\n",
        "  #Setup the model layers\n",
        "  model = tf.keras.Sequential([\n",
        "      hub.KerasLayer(model_url), #Input Layer\n",
        "      tf.keras.layers.Dense(units=1000, activation ='relu'), #Hidden layer\n",
        "      tf.keras.layers.Dropout(0.5),\n",
        "      tf.keras.layers.Dense(units=output_shape,activation='softmax') #Output layer\n",
        "                              ])\n",
        "\n",
        "  #Compiling the model\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "      optimizer=tf.keras.optimizers.Adam(),\n",
        "      metrics= ['accuracy']\n",
        "  )\n",
        "  #Building the model\n",
        "  model.build(input_shape)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOMpr0F3mlR_"
      },
      "outputs": [],
      "source": [
        "model = create_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOAMcYbBm1_c"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jK4vIdN1QNx"
      },
      "source": [
        "## Creating callbacks\n",
        "\n",
        "Callbacks are helper functions that a model uses during training to save/check its progress or stop training early if the model stops improving.\n",
        "\n",
        "We'll create two callbacks, one for TensorBoard, which helps track our models progress, and another for early stopping, which prevents our model from training for too long."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFHXZ9HQ6A_0"
      },
      "source": [
        "### Tensorboard callback\n",
        "three things are needed:\n",
        "1. Load up the tensorboard notebook extension\n",
        "2. Create a tensorboard callback that can save logs to a directory\n",
        "3. Pass the tensorboard callback to the model's fit function\n",
        "4. Visualize the model's training log with the %tensorbard magic *function*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m51gB2yTx1YO"
      },
      "outputs": [],
      "source": [
        "#Load tensorboard notebook\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWgUGnFH6u4p"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "#Create a function to build a tensorboard callback\n",
        "def create_tensorboard_callback():\n",
        "  #Create a log directory for storing tensorboard logs\n",
        "  logdir = os.path.join('/content/drive/MyDrive/Dog breed classification project/logs',\n",
        "                        #make it so that the logs get tracked whenever we train the model\n",
        "                        datetime.datetime.now().strftime(\"%d%m%Y-%H%M%S\"))\n",
        "  return tf.keras.callbacks.TensorBoard(log_dir=logdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz3E_fs08a66"
      },
      "source": [
        "### Creating an early stopping callback\n",
        "\n",
        "This callback stops the model from overfitting by stopping training if a certain evaluation metric stops improving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVBeh5b09LuG"
      },
      "outputs": [],
      "source": [
        "#Creating an early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIdUMXJs9xMq"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "The first model will only train on 1000 images to make sure that everything is working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atBYQEI59hZM"
      },
      "outputs": [],
      "source": [
        "#Number of epochs\n",
        "EPOCHS = 100 #@param {type:'slider', min:10, max:100, step:10}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaKgLU3Bf8pW"
      },
      "outputs": [],
      "source": [
        "#Checking to make sure the GPU is available\n",
        "tf.config.list_physical_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC1qNGJgiH83"
      },
      "source": [
        "Creating a function that trains the model\n",
        "\n",
        "* Creates a model with the create_model function\n",
        "* Creates a tensorboard callback with create_tensorboard_callback\n",
        "* Calls the fit function of the model\n",
        "* Returns the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G51bXWQEhBnL"
      },
      "outputs": [],
      "source": [
        "#Function to create, train, and return a trained model\n",
        "\n",
        "def train_model():\n",
        "  #Create model\n",
        "  model = create_model()\n",
        "\n",
        "  #Create a new tensorboard callback every time we train a model\n",
        "  tensorboard = create_tensorboard_callback()\n",
        "\n",
        "  #Train the model\n",
        "  model.fit(x=train_data,\n",
        "            epochs= EPOCHS,\n",
        "            validation_data=val_data,\n",
        "            validation_freq=1,\n",
        "            callbacks= [tensorboard,early_stopping])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpLyfob3plee"
      },
      "outputs": [],
      "source": [
        "#Fit model to the data\n",
        "model = train_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making and evaluating predictions with a trained model"
      ],
      "metadata": {
        "id": "z-iK9HeDQRgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(val_data, verbose=1)"
      ],
      "metadata": {
        "id": "XQ1sRK7HQWFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Each prediction is array with 120 values, one for the probability of each class\n",
        "predictions[0]"
      ],
      "metadata": {
        "id": "LuPoSXZubgSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for getting the labels out of a prediction\n",
        "def get_prediction_label(prediction):\n",
        "  '''\n",
        "  Turns an array of prediction probabilities into a label\n",
        "  '''\n",
        "  return unique_breeds[np.argmax(prediction)]\n",
        "\n",
        "#example\n",
        "get_prediction_label(predictions[0])"
      ],
      "metadata": {
        "id": "OqEsrkDbwCQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a function to unbatch a batched dataset\n",
        "def unbatch_dataset(dataset):\n",
        "  '''\n",
        " Takes a batched data set, unbatches it,\n",
        " and returns lists of the images and labels\n",
        " '''\n",
        "  ub_images = []\n",
        "  ub_labels = []\n",
        "  for image, label in dataset.unbatch().as_numpy_iterator():\n",
        "    ub_images.append(image)\n",
        "    ub_labels.append(label)\n",
        "  return ub_images, ub_labels"
      ],
      "metadata": {
        "id": "nkGt0nnYz0lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_images, val_labels = unbatch_dataset(val_data)"
      ],
      "metadata": {
        "id": "VN3_rRuRWdi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create a function that plots the predicted label, its predicted probability, and the target image on a single plot."
      ],
      "metadata": {
        "id": "PRVmoovCZ3dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_prediction(pred_prob, true_label, image, n=0):\n",
        "\n",
        "  #pred label\n",
        "  pred_label = get_prediction_label(pred_prob[n])\n",
        "  actual_label = unique_breeds[true_label[n].argmax()]\n",
        "\n",
        "  #Color title depending on prediction correctness\n",
        "  if pred_label == actual_label:\n",
        "    color = 'green'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  #plot image and remove image\n",
        "  plt.imshow(image[n])\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.title(f\"True breed: {actual_label},  predicted breed: {pred_label},  probability: {np.max(pred_prob[n])*100:.2f}%\", color=color, fontsize=10)\n"
      ],
      "metadata": {
        "id": "1_FRodOYYEIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The prediction is correct, even though the probability is low\n",
        "#This means the model is not too sure\n",
        "plot_prediction(predictions,val_labels, val_images, n=1)"
      ],
      "metadata": {
        "id": "nPAqQf7OdAhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a function that plots the top ten prediction confidences for a single prediction\n",
        "def plot_pred_conf(predictions, labels, n=1):\n",
        "  pred_prob, true_label = predictions[n], labels[n]\n",
        "\n",
        "  #The predicted label\n",
        "  pred_label = get_prediction_label(pred_prob)\n",
        "\n",
        "  #top ten prediction confidence indexes\n",
        "  top_ten_indexes = pred_prob.argsort()[-10:][::-1] #[-10:] gets the last ten and [::-1] orders them in descending order\n",
        "  #top ten prediction confidence values\n",
        "  top_ten_values = pred_prob[top_ten_indexes]\n",
        "  #top ten prediction labels\n",
        "  top_ten_labels = unique_breeds[top_ten_indexes]\n",
        "\n",
        "  #setup plot\n",
        "  top_plot = plt.bar(np.arange(len(top_ten_labels)),top_ten_values, color='grey')\n",
        "  plt.xticks(np.arange(len(top_ten_labels)), labels=top_ten_labels, rotation='vertical')\n",
        "\n",
        "  #Change the color of the true label\n",
        "  if np.isin(unique_breeds[true_label.argmax()], top_ten_labels):\n",
        "    top_plot[np.argmax(top_ten_labels == unique_breeds[true_label.argmax()])].set_color(\"green\")\n",
        "  else:\n",
        "    pass"
      ],
      "metadata": {
        "id": "O5udqEZ4nBjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Breed probability values for the previous image\n",
        "plot_pred_conf(predictions, val_labels, n=1)"
      ],
      "metadata": {
        "id": "jd9pqg3Kzt6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.isin(5,np.array([0,1,2,3,4,5]))"
      ],
      "metadata": {
        "id": "0X5M2MAX0567"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the images along with the top ten predictions\n",
        "i_multiplier = 30\n",
        "num_rows = 3\n",
        "num_columns = 2\n",
        "num_images = num_rows*num_columns\n",
        "\n",
        "plt.figure(figsize=(5*2*num_columns, 5*num_rows))\n",
        "\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows,2*num_columns, 2*i+1)\n",
        "  plot_prediction(predictions,val_labels,val_images, n=i+i_multiplier)\n",
        "  plt.subplot(num_rows,2*num_columns, 2*i+2)\n",
        "  plot_pred_conf(predictions,val_labels,n=i+i_multiplier)\n",
        "\n",
        "plt.tight_layout(h_pad=1.0)"
      ],
      "metadata": {
        "id": "u3v7L0l-5K9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a confusion matrix with this model"
      ],
      "metadata": {
        "id": "oP9ed7gMCspy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funcions for aving and loading the model"
      ],
      "metadata": {
        "id": "ejgc_5rGDL4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, suffix=None):\n",
        "  \"\"\"\n",
        "  Saves a given model in a models directory and appends a suffix (str)\n",
        "  for clarity and reuse.\n",
        "  \"\"\"\n",
        "  # Create model directory with current time\n",
        "  modeldir = os.path.join(\"/content/drive/MyDrive/Dog breed classification project/models\",\n",
        "                          datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\"))\n",
        "  model_path = modeldir + \"-\" + suffix + \".h5\" # save format of model\n",
        "  print(f\"Saving model to: {model_path}...\")\n",
        "  model.save(model_path)\n",
        "  return model_path"
      ],
      "metadata": {
        "id": "onDcLHUoDR2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path):\n",
        "  \"\"\"\n",
        "  Loads a saved model from a specified path.\n",
        "  \"\"\"\n",
        "  print(f\"Loading saved model from: {model_path}\")\n",
        "  model = tf.keras.models.load_model(model_path,\n",
        "                                     custom_objects={\"KerasLayer\":hub.KerasLayer})\n",
        "  return model"
      ],
      "metadata": {
        "id": "2GO7aiteEYl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a model on the full data"
      ],
      "metadata": {
        "id": "KVED2JigH5K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(y)"
      ],
      "metadata": {
        "id": "btC3hGyE4sVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating batches of the full data\n",
        "full_data = create_data_batches(X,y)"
      ],
      "metadata": {
        "id": "yDDFhonVFe4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiating a new model\n",
        "full_model = create_model()"
      ],
      "metadata": {
        "id": "DDgmD8dd42no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the full model callbacks\n",
        "#tensorboard\n",
        "full_model_tensorboard = create_tensorboard_callback()\n",
        "#early stopping\n",
        "full_early_stop = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)"
      ],
      "metadata": {
        "id": "04XlpkUd5Bg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of epochs\n",
        "NUM_EPOCHS = 100 #@param {type:\"slider\", min:10, max:100, step:10}"
      ],
      "metadata": {
        "id": "FjefpFsU6YRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the full data model\n",
        "full_model.fit(x=full_data,\n",
        "               epochs=NUM_EPOCHS,\n",
        "               callbacks=[full_model_tensorboard, full_early_stop])"
      ],
      "metadata": {
        "id": "K8EeCoXf5oIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the full model\n",
        "save_model(full_model,'full_dataset')"
      ],
      "metadata": {
        "id": "HGjzAOlBFeeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the full model\n",
        "full_model = load_model('/content/drive/MyDrive/Dog breed classification project/models/20240325-23381711409907-full_dataset.h5')"
      ],
      "metadata": {
        "id": "20dmeWNCFvJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_model.summary()"
      ],
      "metadata": {
        "id": "LtWWtcIpGa5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making predictions on the test set\n",
        "\n",
        "The test data has to be put into batches because that is what the model trained on."
      ],
      "metadata": {
        "id": "wlFTAivwGqly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load test image filenames\n",
        "test_path = '/content/drive/MyDrive/Dog breed classification project/test/'\n",
        "test_filenames = [test_path + fname for fname in os.listdir(test_path)]\n",
        "test_filenames[:5]"
      ],
      "metadata": {
        "id": "wotE5qg3GtLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating data batches of the test set\n",
        "test_data = create_data_batches(test_filenames, test_data = True)"
      ],
      "metadata": {
        "id": "Vjjc_RusJYSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "Y4BZuJxBKMRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = full_model.predict(test_data, verbose=1)"
      ],
      "metadata": {
        "id": "KhtHOfSjK4Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save predictions\n",
        "np.savetxt('/content/drive/MyDrive/Dog breed classification project/test-predictions.csv', test_predictions, delimiter=',')"
      ],
      "metadata": {
        "id": "p3ywEMGXLvZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load predictions\n",
        "test_predictions = np.loadtxt('/content/drive/MyDrive/Dog breed classification project/test-predictions.csv', delimiter=',')"
      ],
      "metadata": {
        "id": "O-QHXE3rP1lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing prediction set for Kaggle\n",
        "\n",
        "We must create a csv with the id of the image and the predicted probability for each unique breed."
      ],
      "metadata": {
        "id": "HS_9LRVlS_50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting the columns of the dataframe\n",
        "df = pd.DataFrame(columns=['id']+list(unique_breeds))"
      ],
      "metadata": {
        "id": "EW-1jhHITIll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Append test image id's to the prediction DataFrame\n",
        "test_ids = [os.path.splitext(path)[0] for path in os.listdir(test_path)]\n",
        "df['id'] = test_ids"
      ],
      "metadata": {
        "id": "XQcUOe7Y2V5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add the prediction probabilities to each dob breed column\n",
        "df[list(unique_breeds)] = test_predictions"
      ],
      "metadata": {
        "id": "KfK53JmL33oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "br2pBbPO6ATV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the prediction\n",
        "df.to_csv('/content/drive/MyDrive/Dog breed classification project/full_model_predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "8KDx8udR6BHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making predictions on custom images\n",
        "\n",
        "The steps are the same as those for the test set.\n",
        "Steps:\n",
        "* Get the paths\n",
        "* Create batches with the images\n",
        "* input the batch to the predict function of the model\n",
        "* convert the prediction to labels"
      ],
      "metadata": {
        "id": "4INwi41FFCH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating custom filepaths\n",
        "custom_path = '/content/drive/MyDrive/Dog breed classification project/Custom images/'\n",
        "custom_filepaths = [custom_path + fnames for fnames in os.listdir(custom_path)]\n",
        "custom_filepaths"
      ],
      "metadata": {
        "id": "dplzzLJsOs33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inputting the filepath directly\n",
        "custom_data = create_data_batches(custom_filepaths, test_data = True)"
      ],
      "metadata": {
        "id": "NChJgMNaFAnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_predictions = full_model.predict(custom_data)"
      ],
      "metadata": {
        "id": "qJhdEsKLJCSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_prediction_label(custom_predictions[0])"
      ],
      "metadata": {
        "id": "DcudaAeBJUri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_pred_labels = [get_prediction_label(custom_predictions[i]) for i in range(len(custom_predictions))]"
      ],
      "metadata": {
        "id": "Uo6AnK5Rjp6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_pred_labels"
      ],
      "metadata": {
        "id": "ibBqsDOknE0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loop through unbatched data\n",
        "custom_images = []\n",
        "for image in custom_data.unbatch().as_numpy_iterator():\n",
        "  custom_images.append(image)\n",
        "\n",
        "custom_images"
      ],
      "metadata": {
        "id": "bQ7CPPX8ndb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(custom_images[0])\n",
        "plt.title(f'{np.max(custom_predictions[0])*100:.2f}% {custom_pred_labels[0]}')\n",
        "plt.xticks([])\n",
        "plt.yticks([]);"
      ],
      "metadata": {
        "id": "fkfnksykoVt8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}